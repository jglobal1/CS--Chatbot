{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FUT QA Assistant - Model Training\n",
        "\n",
        "This notebook trains a fine-tuned question-answering model for the Federal University of Technology (FUT) QA Assistant system.\n",
        "\n",
        "## Steps:\n",
        "1. Install dependencies\n",
        "2. Mount Google Drive\n",
        "3. Load and prepare training data\n",
        "4. Train the model\n",
        "5. Save the trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "!pip install transformers torch datasets accelerate evaluate\n",
        "!pip install fastapi uvicorn pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Google Drive mounted successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForQuestionAnswering, \n",
        "    TrainingArguments, Trainer, DefaultDataCollator\n",
        ")\n",
        "from datasets import Dataset\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"distilbert/distilbert-base-cased-distilled-squad\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/fut_qa_model\"\n",
        "MAX_LENGTH = 512\n",
        "STRIDE = 128\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"Max length: {MAX_LENGTH}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(\"Tokenizer loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample training data - Replace this with your actual FUT data\n",
        "sample_data = [\n",
        "    {\n",
        "        \"context\": \"Federal University of Technology (FUT) is a Nigerian university focused on technology and engineering education. The university offers various programs in engineering, technology, and applied sciences.\",\n",
        "        \"question\": \"What is FUT known for?\",\n",
        "        \"answers\": {\n",
        "            \"text\": [\"technology and engineering education\"],\n",
        "            \"answer_start\": [45]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"The Computer Science department at FUT offers programs in software engineering, artificial intelligence, and data science. Students learn programming languages like Python, Java, and C++.\",\n",
        "        \"question\": \"What programming languages are taught in Computer Science?\",\n",
        "        \"answers\": {\n",
        "            \"text\": [\"Python, Java, and C++\"],\n",
        "            \"answer_start\": [145]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"FUT has multiple campuses and offers both undergraduate and postgraduate programs. The university is known for its practical approach to education and industry partnerships.\",\n",
        "        \"question\": \"What types of programs does FUT offer?\",\n",
        "        \"answers\": {\n",
        "            \"text\": [\"undergraduate and postgraduate programs\"],\n",
        "            \"answer_start\": [45]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Sample data created with {len(sample_data)} examples\")\n",
        "print(\"\\nSample example:\")\n",
        "print(f\"Context: {sample_data[0]['context']}\")\n",
        "print(f\"Question: {sample_data[0]['question']}\")\n",
        "print(f\"Answer: {sample_data[0]['answers']['text'][0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to load your actual training data\n",
        "def load_training_data():\n",
        "    \"\"\"\n",
        "    Load training data from Google Drive or create sample data\n",
        "    \"\"\"\n",
        "    # Check multiple possible locations for your training data\n",
        "    possible_paths = [\n",
        "        \"/content/drive/MyDrive/fut_training_data_for_colab.json\",\n",
        "        \"/content/drive/MyDrive/fut_qa_training_data.json\",\n",
        "        \"/content/drive/MyDrive/fut_comprehensive_training_data.json\"\n",
        "    ]\n",
        "    \n",
        "    for data_path in possible_paths:\n",
        "        if os.path.exists(data_path):\n",
        "            print(f\"Loading training data from {data_path}\")\n",
        "            with open(data_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            print(f\"‚úÖ Loaded {len(data)} training examples\")\n",
        "            return data\n",
        "    \n",
        "    print(\"No training data found in Google Drive, using sample data\")\n",
        "    print(\"üìÅ To use your own data:\")\n",
        "    print(\"1. Upload 'fut_training_data_for_colab.json' to your Google Drive\")\n",
        "    print(\"2. Make sure it's in the root of your Google Drive\")\n",
        "    print(\"3. Re-run this cell\")\n",
        "    return sample_data\n",
        "\n",
        "# Load training data\n",
        "training_data = load_training_data()\n",
        "print(f\"Loaded {len(training_data)} training examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data preprocessing functions\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"\n",
        "    Preprocess the training data for the model\n",
        "    \"\"\"\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=MAX_LENGTH,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_offsets_mapping=True,\n",
        "    )\n",
        "    \n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while idx < len(sequence_ids) and sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs\n",
        "\n",
        "print(\"Data preprocessing functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare datasets\n",
        "print(\"Preparing datasets...\")\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "dataset = Dataset.from_list(training_data)\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "eval_dataset = train_test_split[\"test\"]\n",
        "\n",
        "# Apply preprocessing\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        ")\n",
        "\n",
        "eval_dataset = eval_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=eval_dataset.column_names,\n",
        ")\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
        "print(\"Datasets prepared successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DefaultDataCollator()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured!\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "print(\"This may take several minutes depending on your data size and hardware.\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"Training completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "print(\"Saving the trained model...\")\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Save the model and tokenizer\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"Model saved successfully to {OUTPUT_DIR}\")\n",
        "print(\"You can now use this model in your FastAPI backend!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the trained model\n",
        "print(\"Testing the trained model...\")\n",
        "\n",
        "# Load the trained model\n",
        "trained_model = AutoModelForQuestionAnswering.from_pretrained(OUTPUT_DIR)\n",
        "trained_tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Create a pipeline for testing\n",
        "qa_pipeline = pipeline(\"question-answering\", model=trained_model, tokenizer=trained_tokenizer)\n",
        "\n",
        "# Test with a sample question\n",
        "test_context = \"Federal University of Technology (FUT) is a Nigerian university focused on technology and engineering education. The university offers various programs in engineering, technology, and applied sciences.\"\n",
        "test_question = \"What is FUT known for?\"\n",
        "\n",
        "result = qa_pipeline(question=test_question, context=test_context)\n",
        "\n",
        "print(f\"Question: {test_question}\")\n",
        "print(f\"Answer: {result['answer']}\")\n",
        "print(f\"Confidence: {result['score']:.4f}\")\n",
        "\n",
        "print(\"\\nModel testing completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
